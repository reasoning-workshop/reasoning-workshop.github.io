<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Schedule | Foundations of Reasoning in Language Models </title> <meta name="author" content="Foundations of Reasoning in Language Models "> <meta name="description" content="The session will cover invited talks, contributed talks and posters. The tentative schedule in Central European Summer Time (GMT+2) can be found below."> <meta name="keywords" content="NeurIPS, Workshop, Reasoning, Language Models, AI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://reasoning-workshop.github.io/icml2024/schedule/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Foundations of Reasoning in Language Models </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cfp/">Call for Papers </a> </li> <li class="nav-item active"> <a class="nav-link" href="/icml2024/schedule/">Schedule <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/speakers/">Speakers </a> </li> <li class="nav-item "> <a class="nav-link" href="/organizers/">Organizers </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Schedule</h1> <p class="post-description">The session will cover invited talks, contributed talks and posters. The tentative schedule in Central European Summer Time (GMT+2) can be found below.</p> </header> <article> <p><br></p> <div> <table class="table" id="standings" style="border-collapse:collapse"> <tr class="header" style="background-color:rgb(215, 215, 215); border-top: 1pt solid white; border-bottom: 1pt solid black;"> <th style="border-top-left-radius: 10px; width: 15%">Time</th> <th style="width: 15%">Type</th> <th style="width: 70% border-top-right-radius: 10px;">Title &amp; Speakers</th> </tr> <tr> </tr> <tr class="header" style="cursor: pointer"> <td>9:00 a.m.</td> <td> Opening Remarks <br> [<a href="https://slideslive.com/39022170" rel="external nofollow noopener" target="_blank">video</a>] </td> <td> Alberto Metelli (Politecnico di Milano) </td> </tr> <tr class="header" style="cursor: pointer"> <td>9:05 a.m.</td> <td> Invited Talk <br> [<a>slides</a>] [<a href="https://slideslive.com/39022177" rel="external nofollow noopener" target="_blank">video</a>] </td> <td> <i>The Rise of Reinforcement Learning: from One to Many</i> <br> <b>Niao He</b> (ETH Zurich) <br><br> Reinforcement learning (RL), combined with deep neural networks, is key to the boom of recent AI breakthroughs from game mastery to control automation. However, their successes are overly reliant on brute-force computing power and engineering tricks, leaving wide gaps between practice and theory. The lack of theoretical foundations is even more pronounced as we shift from single-agent to many-agent RL, in addressing complex dynamic systems and decision making such as resource allocation, traffic management, and social interaction. The challenges inherent in learning many-agent systems stem not only from the increased computational and strategic complexities but also from practical limitations in coordination and exploration. In this talk, I will shed light on promising principles that break the curses of many-agent RL, focusing on mean-field approximation theory, statistical complexity, and independent learning. This will further pave the way for scalable and principled solutions to unlock the full potential of RL for next-generation AI. </td> </tr> <tr class="header" style="cursor: pointer"> <td>10:00 a.m.</td> <td> Invited Talk <br> [<a>slides</a>] [<a href="https://slideslive.com/39022178" rel="external nofollow noopener" target="_blank">video</a>] </td> <td> <i>Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning</i> <br> <b>Dylan Foster</b> (Microsoft Research) <br><br> Imitation learning (IL) aims to mimic the behavior of an expert in a sequential decision making task by learning from demonstrations, and has been widely applied to robotics, autonomous driving, and autoregressive language generation. The simplest approach to IL, behavior cloning (BC), is thought to incur sample complexity with unfavorable quadratic dependence on the problem horizon, motivating a variety of different online algorithms that attain improved linear horizon dependence under stronger assumptions on the data and the learnerâ€™s access to the expert. <br> In this talk, we revisit the apparent gap between offline and online IL from a learning-theoretic perspective, with a focus on general policy classes up to and including deep neural networks. Through a new analysis of behavior cloning with the logarithmic loss, we will show that it is possible to achieve horizon-independent sample complexity in offline IL whenever (i) the range of the cumulative payoffs is controlled, and (ii) an appropriate notion of supervised learning complexity for the policy class is controlled. When specialized to stationary policies, this implies that the gap between offline and online IL is not fundamental. We will then discuss implications of this result and investigate the extent to which it bears out empirically. </td> </tr> <tr class="header"> <td>10:45 a.m.</td> <td></td> <td>Break</td> </tr> <tr class="header" style="cursor: pointer"> <td>11:00 a.m.</td> <td> Invited Talk <br> [<a>slides</a>] [<a href="https://slideslive.com/39022179" rel="external nofollow noopener" target="_blank">video</a>] </td> <td> <i>Reinforcement Learning at the Hyperscale</i> <br> <b>Jakob Foerster</b> (University of Oxford) <br><br> Deep reinforcement learning is currently undergoing a revolution of scale, fuelled by jointly running the environment, data collection, and training loop on the GPU, which has resulted in orders of magnitude of speed-up for many tasks. <br> In this talk I start by presenting examples of our recent work which have been enabled by this revolution, spanning multi-agent RL, meta-learning, and environment discovery. I will end the talk by outlining failure modes of relying on GPU accelerated environments and possible paradigms for the community to collectively address them, ranging from promising research directions to novel evaluation protocols. </td> </tr> <tr class="header"> <td>11:45 a.m.</td> <td> Contributed Talks <br> [<a href="https://slideslive.com/39022180" rel="external nofollow noopener" target="_blank">video</a>] </td> <td> <i>Is Value Learning Really the Main Bottleneck in Offline RL?</i> <br> <b>Seohong Park</b> (UC Berkeley) <br><br> <i>REBEL: Reinforcement Learning via Regressing Relative Rewards</i> <br> <b>Gokul Swamy</b> (Cornell University) <br><br> <i>Partially Observable Multi-Agent Reinforcement Learning using Mean Field Control</i> <br> <b>Kai Cui</b> (TU Darmstadt) <br><br> <i>Information Theoretic Guarantees For Policy Alignment in Large Language Models</i> <br> <b>Youssef Mroueh</b> (IBM Research) </td> </tr> <tr class="header"> <td>12:25 p.m.</td> <td></td> <td>Lunch Break</td> </tr> <tr class="header" style="background-color:rgb(240, 240, 240);"> <td>1:25 p.m.</td> <td></td> <td> <b>Poster Session 1</b> </td> </tr> <tr class="header" style="background-color:rgb(240, 240, 240);"> <td>2:25 p.m.</td> <td> Panel Discussion <br> [<a href="https://slideslive.com/39022181" rel="external nofollow noopener" target="_blank">video</a>] </td> <td> Moderator: <b>Csaba Szepesvari</b> (University of Alberta) <br> <b>Marcello Restelli</b> (Politecnico di Milano), <b>Sergey Levine</b> (UC Berkeley), <b>Akshay Krishnamurthy</b> (Microsoft Research), <b>Martha White</b> (University of Alberta) </td> </tr> <tr class="header"> <td>3:25 p.m.</td> <td></td> <td>Coffee Break</td> </tr> <tr class="header"> <td>3:40 p.m.</td> <td> Contributed Talks <br> [<a href="https://slideslive.com/39022182" rel="external nofollow noopener" target="_blank">video</a>] </td> <td> <i>A Unified Confidence Sequence for Generalized Linear Models, with Applications to Bandits</i> <br> <b>Kwang-Sung Jun</b> (University of Arizona) <br><br> <i>Transductive Active Learning with Application to Safe Bayesian Optimization</i> <br> <b>Jonas HÃ¼botter</b> (ETH Zurich) </td> </tr> <tr class="header" style="background-color:rgb(240, 240, 240);"> <td>4:00 p.m.</td> <td></td> <td><b>Poster Session 2</b></td> </tr> </table> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Foundations of Reasoning in Language Models . </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>